<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Curriculum-based Reinforcement Learning to Pre-Grasp Floating Target _ IEEE Transaction on Robotics (TRO) _ International Conference on Robotics and Automation (ICRA) 2024, Yokohama, Japan, Space Robotics, Grasping, Reinforcement Learning, Robotiq 3F Gripper, Universal Robots UR-10 UR-5, Camera, Force-Torque Sensor, Zero Gravity, Microgravity, Soft Actor-Critic, Off-Policy, Reward Function, Pre-Grasping, Approach Task, Real-World, Simulated, Experiments, Bahador Beigomi, Zheng H. Zhu, York University, Toronto, Canada, ORCID">
  <meta name="keywords" content="IEEE Transaction on Robotics (TRO), ICRA, Robotics, Grasping, Yokohama, Japan, Reinforcement Learning, Robotiq 3F Gripper, Universal Robots UR-10 UR-5, Camera, Force-Torque Sensor, Zero Gravity, Microgravity, Soft Actor-Critic, Off-Policy, Reward Function, Pre-Grasping, Approach Task, Real-World, Simulated, Experiments, Bahador Beigomi, Zheng H. Zhu, York University, Toronto, Canada, ORCID">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Curriculum-based Reinforcement Learning to Pre-Grasp Floating Target _ IEEE Transaction on Robotics (TRO) _ International Conference on Robotics and Automation (ICRA) 2024, Yokohama, Japan, Space Robotics, Grasping, Reinforcement Learning, Robotiq 3F Gripper, Universal Robots UR-10 UR-5, Camera, Force-Torque Sensor, Zero Gravity, Microgravity, Soft Actor-Critic, Off-Policy, Reward Function, Pre-Grasping, Approach Task, Real-World, Simulated, Experiments, Bahador Beigomi, Zheng H. Zhu, York University, Toronto, Canada, ORCID </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="image/png" href="images/IEEE_TRO_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> Curriculum-based Reinforcement Learning to Pre-Grasp Floating Target </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://baha2r.github.io/"> Bahador Beigomi</a><sup>1</sup>
              <a href="https://orcid.org/0000-0001-5646-2771" target="_blank">
                <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" alt="ORCID iD" />
              </a>
            </span>
            <span class="author-block">
                <a href="https://github.com/Mohatashem"> Mohatashem Reyaz Makhdoomi</a><sup>2</sup>
                <a href="https://orcid.org/0000-0003-1739-5507" target="_blank">
                  <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" alt="ORCID iD" />
                </a>
            </span>
            <span class="author-block">
                <a href="https://carolmartinez.github.io/"> Carol Martinez</a><sup>2</sup>
                <a href="https://orcid.org/0000-0003-3040-6119" target="_blank">
                  <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" alt="ORCID iD" />
                </a>
            </span>
            <span class="author-block">
                <a href="https://www.uni.lu/snt-en/people/miguel-angel-olivares-mendez/"> Miguel A. Olivares-Mendez</a><sup>2</sup>
                <a href="https://orcid.org/0000-0001-8824-3231" target="_blank">
                  <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" alt="ORCID iD" />
                </a>
            </span>
            <span class="author-block">
              <a href="https://www.yorku.ca/gzhu/">Zheng H. (George) Zhu</a><sup>1</sup>
              <a href="https://orcid.org/0000-0002-0149-0473" target="_blank">
                <img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" alt="ORCID iD" />
              </a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>York University</span>
            <span class="author-block"><sup>2</sup>University of Luxembourg</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- IEEE Link. -->
              <span class="link-block">
                <!-- <a href="https://ieeexplore.ieee.org/document/10610017" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                    <img src="images/IEEE_TRO_logo.png" alt="IEEE Logo" style="width:24px; height:auto;">
                  </span>                
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!-- <a href="https://arxiv.org/abs/2406.06460" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <!-- <a href="https://github.com/baha2r/Fanuc_Robotiq_Grasp" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="images/Luxe24/T10.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
    </div>
  </div>
</section>

<!-- scrolling vids -->
<section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div class="content has-text-justified">
          <p>
            Several test have been conducted to evaluate the performance of the proposed approach in real-world scenarios. In the following videos, 
            the robot arm approaches a floating target and moves to the pre-grasp position. The target is moving and rotating randomly in the space.
            All of the required actions to place the gripper in the pre-grasp position are predicted from the trained agent, receiving all the 
            necessary observations from sensors like optitrack. 
          </p>
        </div>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T01.mp4" type="video/mp4">
            </video>
            <p class="video-description">stationary Target - Example 1</p>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T02.mp4" type="video/mp4">
            </video>
            <p class="video-description">Linear motion - Example 2</p>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T03.mp4" type="video/mp4">
            </video>
            <p class="video-description">Linear motion stopped at the end - Example 3</p>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T07.mp4" type="video/mp4">
            </video>
            <p class="video-description">Tilted target wo ang vel - Example 4</p>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T11.mp4" type="video/mp4">
            </video>
            <p class="video-description">Tilted target with ang vel - Example 5</p>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T17.mp4" type="video/mp4">
            </video>
            <p class="video-description">Tilted target with ang vel - Example 6</p>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pre-positioning a robotic gripper relative to a free-floating, moving target in 6-DoF poses significant challenges, especially when both translation and rotation are involved. 
            Precise pre-positioning is critical for successful grasping, as deviations from the optimal pose may result in missed grasp opportunities or collisions. 
          </p>
          <p>
            This work addresses these challenges by leveraging reinforcement learning with curriculum learning to enhance both the efficiency and adaptability of the robotic agent in dynamic environments. 
            Curriculum learning is used to incrementally increase task complexity, enabling the agent to adapt to unforeseen conditions while maintaining precise control. 
            The proposed approach uses a Soft Actor-Critic algorithm with deterministic policy output, first trained in the PyBullet simulation environment with domain randomization to mitigate sim-to-real transfer issues. 
            The trained policy is then transferred to a physical robot to perform dynamic pre-positioning tasks. 
          </p>
          <p>
            Our results demonstrate that this method enables the robotic gripper to accurately track and maintain the correct pose relative to a 6-DoF moving and rotating target, ensuring collision-free pre-positioning, a crucial precursor to grasping. 
            More detailed results and videos are available at GitHub.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Simulatin environment -->
    <section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Simulation Scenarios</h2>
          <div class="content has-text-justified">
            <p>
              To train our agent in a simulated environment, we used PyBullet, a physics engine that provides a realistic simulation of the robot
               and the micro-gravity environment.

            </p>
          </div>
          <div class="publication-image has-text-centered">
            <video src="images/Luxe24/simulations/Presentation3.mp4" style="max-width:100%; height:auto;" autoplay muted loop controls></video>
          </div>          
          
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-steve">
              <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T00.mp4" type="video/mp4">
              </video>
              <p class="video-description">stationary Target - Example 1</p>
            </div>
            <div class="item item-chair-tp">
              <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T03.mp4" type="video/mp4">
              </video>
              <p class="video-description">Linear motion - Example 2</p>
            </div>
            <div class="item item-shiba">
              <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T02.mp4" type="video/mp4">
              </video>
              <p class="video-description">Linear motion stopped at the end - Example 3</p>
            </div>
            <div class="item item-fullbody">
              <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T04.mp4" type="video/mp4">
              </video>
              <p class="video-description">Tilted target wo ang vel - Example 4</p>
            </div>
            <div class="item item-fullbody">
              <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T06.mp4" type="video/mp4">
              </video>
              <p class="video-description">Tilted target with ang vel - Example 5</p>
            </div>
            <div class="item item-fullbody">
              <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
                <source src="images/Luxe24/simulations/T07.mp4" type="video/mp4">
              </video>
              <p class="video-description">Tilted target with ang vel - Example 6</p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!--/ Simulatin environment -->


    <!-- Paper image with explanation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Lab Setup</h2>
        <div class="publication-image">
          <img src="images/Luxe24/The-ZeroG-lab-facility-at-University-of-Luxembourg_W640.jpg" alt="Short description of the image" style="max-width:100%; height:auto;">
          <p class="image-explanation">To test and implement our approach, we used Zero-G lab facilities in Luxembourg. Equipped with two 
            Universal Robots UR10e robotic arms mounted on Cobotracks rails, the system extends the workspace of the robotic arms for 
            emulating spacecraft motion, Zero-G Lab created a established and unique hardware-in-the-loop (HIL) setup to implement and test 
            the trained agent in real-world setups. The system is equipped with a Robotiq 3F gripper, optitrack cameras, and a well-calibrated motion control system 
            completely built on Robotics Operating System (ROS) and Python. The system is capable of simulating microgravity conditions. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Paper image with explanation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Curriculum complexity levels. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-3"></h2> -->
        <h3 class="title is-4">Curriculum Complexity Levels</h3>
        <div class="content has-text-justified">
          <p>
            Standard reinforcement learning algorithms often struggle to learn complex tasks directly from high-dimensional observations. That is why we use
            curriculum learning to incrementally increase the task complexity and help the agent adapt to unforeseen conditions while maintaining precise
            control over the gripper's motion.
            Four hierarchical complexity levels are designed to train the agent in a simulated environment to cope with tilted rotating target.
            The agent is trained with a curriculum learning approach to incrementally increase the task complexity.
            The task difficulty at each level is parameterized by the targetâ€™s orientation \( \psi \) and angular velocity \( \omega \), both of which 
            increase exponentially across the four levels.
            <!-- : \( \psi _i \in [-\pi \cdot \frac{2^{i-1}}{16} , \pi \cdot \frac{2^{i-1}}{16}] \) and \( \omega _i \in [-\pi \cdot \frac{2^{i-1}}{8} , \pi \cdot \frac{2^{i-1}}{8}] \). -->
          </p>        
        </div>
        <div class="columns is-vcentered interpolation-panel" >
          <div class="column is-3 has-text-centered">
            <div style="display: inline-block; clip-path: inset(0 15% 0 15%); ">
              <video id="dollyzoom" autoplay controls muted loop playsinline style="width: 100%;border-radius: 100%;">
                <source src="images/Luxe24/cl_level/L1.mp4" type="video/mp4">
              </video>
            </div>
            <p>\( L1 \)</p>
          </div> 
          <div class="column is-3 has-text-centered" >
            <div style="display: inline-block; clip-path: inset(0 15% 0 15%); ">
              <video id="dollyzoom" autoplay controls muted loop playsinline style="width: 100%;border-radius: 100%;">
                <source src="images/Luxe24/cl_level/L2.mp4" type="video/mp4">
              </video>
            </div>
            <p>\( L2 \)</p>
          </div> 
          <div class="column is-3 has-text-centered">
            <div style="display: inline-block; clip-path: inset(0 15% 0 15%); ">
              <video id="dollyzoom" autoplay controls muted loop playsinline style="width: 100%;border-radius: 100%;">
                <source src="images/Luxe24/cl_level/L3.mp4" type="video/mp4">
              </video>
            </div>
            <p>\( L3 \)</p>
          </div> 
          <div class="column is-3 has-text-centered">
            <div style="display: inline-block; clip-path: inset(0 15% 0 15%); ">
              <video id="dollyzoom" autoplay controls muted loop playsinline style="width: 100%;border-radius: 100%;">
                <source src="images/Luxe24/cl_level/L4.mp4" type="video/mp4">
              </video>
            </div>
            <p>\( L4 \)</p>
          </div>  
        </div>
        <br/>
      </div>
    </div>
    <!-- Curriculum complexity levels. -->

    <!--/ Two videos. -->
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Off-Script Scenarios</h2>
          <p>
            Since the agent is trained in a simulated environment with domain randomization, it is expected to fail in some 
            real-world scenarios. As shown in the following videos, the agent occasionally fails to predict the correct actions 
            needed to place the gripper in the pre-grasp position, but it still manages to get in contact with the target object.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="images/Luxe24/T13.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column">
        <h2 class="title is-3">..........................................</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The agent might be able to recover from the failure, and continue the task, like in simulation environment, 
              but because of the safety reasons, the task is stopped right after the contact happens.
              The main reason for the failure is the agent is not trained with the exact same conditions as the real-world.
            </p>
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="images/Luxe24/T12.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Two videos. -->

    <!-- Reward Shaping. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"></h2>
        <h3 class="title is-4">Reward Shaping</h3>
        <div class="content has-text-justified">
          <p>
            Four novel reward function components designed to drive the robot toward the target, 
            follow it, and reach the pre-grasp position. Each component, normalized between 0 and 1, 
            helps isolate its impact on the robot's behavior during training.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="images/position_reward.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>position correction </p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="images/orientation_reward.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">orientation correction</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="images/top_reward.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">topology encouragement</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="images/contact_reward.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">contact penalization</p>
          </div>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Reward Shaping. -->


    <!-- Related Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@INPROCEEDINGS{10610017,
      <!-- author={Beigomi, Bahador and Zhu, Zheng H.},
      booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
      title={Towards Real-World Efficiency: Domain Randomization in Reinforcement Learning for Pre-Capture of Free-Floating Moving Targets by Autonomous Robots}, 
      year={2024},
      volume={},
      number={},
      pages={11753-11759},
      keywords={Training;Target tracking;Source coding;Reinforcement learning;Robustness;Sensors;Grippers},
      doi={10.1109/ICRA57147.2024.10610017}} -->
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">Here</a>.
            <!-- This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
